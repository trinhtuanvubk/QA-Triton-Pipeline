version: "3"

services:
  triton_server: 
    image: nvcr.io/nvidia/tritonserver:21.10-py3
    shm_size: '2gb' #<-- when RUNNING
    container_name: triton_server
    restart: unless-stopped
    hostname: triton_server
    ports:
      - "8030-8032:8000-8002"
    environment:
      - HOME=/config 
    volumes:
      - ./model_repository/:/models
    command: bash -c "tritonserver --model-repository=/models --log-verbose 1"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]